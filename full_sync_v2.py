#!/usr/bin/env python3
"""
å…¨é‡è‚¡ç¥¨åŒæ­¥è„šæœ¬ v2

ä»Flaskè·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œå¾ªç¯è°ƒç”¨Flaskæ¥å£åŒæ­¥æ¯åªè‚¡ç¥¨çš„å†å²è¡Œæƒ…
æ”¯æŒæµ‹è¯•æ¨¡å¼ï¼šæŒ‡å®šå•åªè‚¡ç¥¨è¿›è¡ŒåŒæ­¥
"""

import requests
import time
import logging
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional

# é…ç½®æ—¥å¿—
log_dir = Path('logs')
log_dir.mkdir(exist_ok=True)

log_file = log_dir / f'full_sync_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)
logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")


class FullSyncClient:
    """å…¨é‡åŒæ­¥å®¢æˆ·ç«¯"""
    
    def __init__(self, 
                 sync_url: str = "http://localhost:7777/api",
                 query_url: str = "http://localhost:8000/api"):
        """
        åˆå§‹åŒ–åŒæ­¥å®¢æˆ·ç«¯
        
        Args:
            sync_url: åŒæ­¥æœåŠ¡URLï¼ˆç«¯å£7777ï¼‰
            query_url: æŸ¥è¯¢æœåŠ¡URLï¼ˆç«¯å£8000ï¼‰
        """
        self.sync_url = sync_url
        self.query_url = query_url
        self.session = requests.Session()
        self.session.headers.update({
            'Content-Type': 'application/json',
            'User-Agent': 'Full Sync Script v2/1.0'
        })
        logger.info(f"ğŸ“ åŒæ­¥æœåŠ¡åœ°å€: {self.sync_url}")
        logger.info(f"ğŸ“ æŸ¥è¯¢æœåŠ¡åœ°å€: {self.query_url}")
    
    def get_all_stocks(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰è‚¡ç¥¨åˆ—è¡¨ï¼ˆä»æŸ¥è¯¢æœåŠ¡ï¼‰- ä½¿ç”¨å¤§limitè·å–æ‰€æœ‰è‚¡ç¥¨"""
        try:
            # ä½¿ç”¨è¶³å¤Ÿå¤§çš„limitå€¼ä¸€æ¬¡æ€§è·å–æ‰€æœ‰è‚¡ç¥¨
            params = {
                'limit': 10000,  # APIæœ€å¤§æ”¯æŒ10000æ¡
                'is_active': 'true'  # åªè·å–æ´»è·ƒè‚¡ç¥¨
            }
            
            logger.info("ğŸ“Š å¼€å§‹è·å–è‚¡ç¥¨åˆ—è¡¨...")
            response = self.session.get(
                f"{self.query_url}/stock-info/local",
                params=params
            )
            response.raise_for_status()
            result = response.json()

            # æ£€æŸ¥APIå“åº”æ ¼å¼ (Flask APIè¿”å› code=200 è¡¨ç¤ºæˆåŠŸ)
            if result.get('code') == 200:
                stocks = result.get('data', [])
                # è½¬æ¢æ•°æ®æ ¼å¼ï¼Œå°†tickerè½¬æ¢ä¸ºsymbolæ ¼å¼
                converted_stocks = []
                for stock in stocks:
                    # å°†APIè¿”å›çš„æ ¼å¼è½¬æ¢ä¸ºè„šæœ¬æœŸæœ›çš„æ ¼å¼
                    market_prefix = {
                        'XSHG': 'SH',
                        'XSHE': 'SZ',
                        'BJSE': 'BJ'
                    }.get(stock.get('exchange_code', 'XSHG'), 'SH')

                    converted_stocks.append({
                        'symbol': f"{market_prefix}.{stock.get('ticker', '')}",
                        'stock_name': stock.get('name', ''),
                        'ticker': stock.get('ticker', ''),
                        'exchange_code': stock.get('exchange_code', ''),
                        'is_active': stock.get('is_active', 1),
                        'last_sync_date': stock.get('last_sync_date', 'æ— ')
                    })

                logger.info(f"æˆåŠŸè·å–è‚¡ç¥¨åˆ—è¡¨: {len(converted_stocks)} åªè‚¡ç¥¨")
                return converted_stocks
            else:
                logger.error(f"APIè¿”å›é”™è¯¯: code={result.get('code')}, message={result.get('message')}")
                return []

        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¼‚å¸¸: {e}")
            return []
    
    def sync_single_stock(self, symbol: str) -> Dict[str, Any]:
        """åŒæ­¥å•åªè‚¡ç¥¨ï¼ˆè°ƒç”¨åŒæ­¥æœåŠ¡ï¼‰"""
        try:
            data = {'symbol': symbol}
            
            response = self.session.post(
                f"{self.sync_url}/sync/single-stock",
                json=data,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            response.raise_for_status()
            return response.json()
            
        except Exception as e:
            logger.error(f"åŒæ­¥è‚¡ç¥¨ {symbol} å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def get_etf_list(self) -> List[Dict[str, Any]]:
        """è·å–ETFåˆ—è¡¨ï¼ˆä»æ•°æ®åº“ï¼‰"""
        try:
            import sys
            import os
            sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            
            from database.connection import db_manager
            from models.stock_data import StockInfo
            
            with db_manager.get_session() as session:
                # æŸ¥è¯¢is_etf='Y'çš„è®°å½•
                etfs = session.query(StockInfo).filter(
                    StockInfo.is_etf == 'Y'
                ).all()
                
                etf_list = []
                for etf in etfs:
                    etf_list.append({
                        'symbol': etf.symbol,
                        'stock_name': etf.stock_name,
                        'ticker': etf.ticker,
                        'exchange_code': etf.market_code,
                        'is_active': etf.is_active,
                        'last_sync_date': str(etf.last_sync_date) if etf.last_sync_date else 'æ— '
                    })
                
                logger.info(f"âœ… æˆåŠŸè·å–ETFåˆ—è¡¨: æ€»è®¡ {len(etf_list)} åªETF")
                return etf_list
                
        except Exception as e:
            logger.error(f"è·å–ETFåˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def run_etf_sync(self, max_etfs: Optional[int] = None, skip_count: int = 0):
        """è¿è¡ŒETFä»·æ ¼åŒæ­¥ï¼ˆé€åªåŒæ­¥æ¨¡å¼ï¼‰"""
        
        logger.info("="*70)
        logger.info("ğŸš€ ETFä»·æ ¼åŒæ­¥å¼€å§‹")
        logger.info("="*70)
        
        # è·å–ETFåˆ—è¡¨
        logger.info("ğŸ“Š ä»æ•°æ®åº“è·å–ETFåˆ—è¡¨...")
        etfs = self.get_etf_list()
        
        if not etfs:
            logger.error("âŒ æ•°æ®åº“ä¸­æ²¡æœ‰ETFè®°å½•")
            logger.info("   è¯·å…ˆæ‰§è¡ŒETFåˆ—è¡¨åŒæ­¥:")
            logger.info("   curl -X POST http://localhost:7777/api/sync/etf/lists")
            return
        
        total_etfs = len(etfs)
        logger.info(f"âœ… è·å–åˆ° {total_etfs} åªETF")
        
        # åº”ç”¨é™åˆ¶
        if skip_count > 0:
            etfs = etfs[skip_count:]
            logger.info(f"â­ï¸  è·³è¿‡å‰ {skip_count} åªETF")
        
        if max_etfs:
            etfs = etfs[:max_etfs]
            logger.info(f"ğŸ¯ é™åˆ¶åŒæ­¥æ•°é‡: {max_etfs} åª")
        
        logger.info(f"ğŸ“ˆ å®é™…åŒæ­¥æ•°é‡: {len(etfs)} åª")
        logger.info("="*70)
        
        # ç»Ÿè®¡ä¿¡æ¯
        success_count = 0
        failed_count = 0
        up_to_date_count = 0
        total_inserted = 0
        
        start_time = time.time()
        
        # é€åªåŒæ­¥ETFä»·æ ¼ï¼ˆå¤ç”¨ç°æœ‰çš„sync_single_stockæ–¹æ³•ï¼‰
        for idx, etf in enumerate(etfs, 1):
            symbol = etf['symbol']
            etf_name = etf.get('stock_name', symbol)
            last_sync = etf.get('last_sync_date', 'æ— ')
            
            logger.info(f"\n[{idx}/{len(etfs)}] æ­£åœ¨åŒæ­¥ETF: {symbol} - {etf_name}")
            logger.info(f"          ä¸Šæ¬¡åŒæ­¥: {last_sync}")
            
            # è°ƒç”¨ç›¸åŒçš„single-stockæ¥å£
            result = self.sync_single_stock(symbol)
            
            if result.get('task', {}).get('status') == 'success':
                task_result = result['task']['result']
                action = task_result.get('action')
                
                if action == 'up_to_date':
                    up_to_date_count += 1
                    logger.info(f"          âœ… å·²æ˜¯æœ€æ–°")
                elif action == 'completed':
                    success_count += 1
                    inserted = task_result.get('inserted_count', 0)
                    total_inserted += inserted
                    latest_date = task_result.get('latest_sync_date', '')
                    logger.info(f"          âœ… æˆåŠŸ - æ–°å¢ {inserted} æ¡, æœ€æ–°æ—¥æœŸ: {latest_date}")
                else:
                    success_count += 1
                    logger.info(f"          âœ… å®Œæˆ")
            else:
                failed_count += 1
                error = result.get('task', {}).get('result', {}).get('error', 'æœªçŸ¥é”™è¯¯')
                logger.info(f"          âŒ å¤±è´¥ - {error}")
            
            # è¿›åº¦æç¤ºï¼ˆæ¯10åªï¼‰
            if idx % 10 == 0:
                elapsed = time.time() - start_time
                avg_time = elapsed / idx
                remaining = (len(etfs) - idx) * avg_time
                logger.info(f"\n{'â”€'*70}")
                logger.info(f"è¿›åº¦: {idx}/{len(etfs)} ({idx/len(etfs)*100:.1f}%)")
                logger.info(f"æˆåŠŸ: {success_count}, æœ€æ–°: {up_to_date_count}, å¤±è´¥: {failed_count}")
                logger.info(f"å·²ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ, é¢„è®¡å‰©ä½™: {remaining/60:.1f}åˆ†é’Ÿ")
                logger.info(f"{'â”€'*70}\n")
        
        # æœ€ç»ˆç»Ÿè®¡
        total_time = time.time() - start_time
        
        logger.info(f"\n{'='*70}")
        logger.info("ğŸ‰ ETFä»·æ ¼åŒæ­¥å®Œæˆï¼")
        logger.info(f"{'='*70}")
        logger.info(f"âœ… åŒæ­¥æˆåŠŸ: {success_count} åª")
        logger.info(f"ğŸ“Œ å·²æ˜¯æœ€æ–°: {up_to_date_count} åª")
        logger.info(f"âŒ åŒæ­¥å¤±è´¥: {failed_count} åª")
        logger.info(f"ğŸ“Š æ–°å¢è®°å½•: {total_inserted:,} æ¡")
        logger.info(f"â±ï¸  æ€»ç”¨æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
        if len(etfs) > 0:
            logger.info(f"âš¡ å¹³å‡é€Ÿåº¦: {total_time/len(etfs):.1f} ç§’/ETF")
        logger.info(f"{'='*70}\n")
    
    def run_full_sync(self, max_stocks: Optional[int] = None, skip_count: int = 0):
        """è¿è¡Œå…¨é‡åŒæ­¥"""
        
        logger.info("="*70)
        logger.info("ğŸš€ å…¨é‡è‚¡ç¥¨åŒæ­¥å¼€å§‹")
        logger.info("="*70)
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        logger.info("ğŸ“Š è·å–è‚¡ç¥¨åˆ—è¡¨...")
        stocks = self.get_all_stocks()
        
        if not stocks:
            logger.error("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨")
            return
        
        total_stocks = len(stocks)
        logger.info(f"âœ… è·å–åˆ° {total_stocks} åªè‚¡ç¥¨")
        
        # åº”ç”¨é™åˆ¶
        if skip_count > 0:
            stocks = stocks[skip_count:]
            logger.info(f"â­ï¸  è·³è¿‡å‰ {skip_count} åªè‚¡ç¥¨")
        
        if max_stocks:
            stocks = stocks[:max_stocks]
            logger.info(f"ğŸ¯ é™åˆ¶åŒæ­¥æ•°é‡: {max_stocks} åª")
        
        logger.info(f"ğŸ“ˆ å®é™…åŒæ­¥æ•°é‡: {len(stocks)} åª")
        logger.info("="*70)
        
        # ç»Ÿè®¡ä¿¡æ¯
        success_count = 0
        failed_count = 0
        up_to_date_count = 0
        total_inserted = 0
        
        start_time = time.time()
        
        # é€åªåŒæ­¥
        for idx, stock in enumerate(stocks, 1):
            symbol = stock['symbol']
            stock_name = stock.get('stock_name', symbol)
            last_sync = stock.get('last_sync_date', 'æ— ')
            
            logger.info(f"\n[{idx}/{len(stocks)}] æ­£åœ¨åŒæ­¥: {symbol} - {stock_name}")
            logger.info(f"          ä¸Šæ¬¡åŒæ­¥: {last_sync}")
            
            result = self.sync_single_stock(symbol)
            
            if result.get('task', {}).get('status') == 'success':
                task_result = result['task']['result']
                action = task_result.get('action')
                
                if action == 'up_to_date':
                    up_to_date_count += 1
                    logger.info(f"          âœ… å·²æ˜¯æœ€æ–°")
                elif action == 'completed':
                    success_count += 1
                    inserted = task_result.get('inserted_count', 0)
                    total_inserted += inserted
                    latest_date = task_result.get('latest_sync_date', '')
                    logger.info(f"          âœ… æˆåŠŸ - æ–°å¢ {inserted} æ¡, æœ€æ–°æ—¥æœŸ: {latest_date}")
                else:
                    success_count += 1
                    logger.info(f"          âœ… å®Œæˆ")
            else:
                failed_count += 1
                error = result.get('task', {}).get('result', {}).get('error', 'æœªçŸ¥é”™è¯¯')
                logger.info(f"          âŒ å¤±è´¥ - {error}")
            
            # è¿›åº¦æç¤º
            if idx % 10 == 0:
                elapsed = time.time() - start_time
                avg_time = elapsed / idx
                remaining = (len(stocks) - idx) * avg_time
                logger.info(f"\n{'â”€'*70}")
                logger.info(f"è¿›åº¦: {idx}/{len(stocks)} ({idx/len(stocks)*100:.1f}%)")
                logger.info(f"æˆåŠŸ: {success_count}, æœ€æ–°: {up_to_date_count}, å¤±è´¥: {failed_count}")
                logger.info(f"å·²ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ, é¢„è®¡å‰©ä½™: {remaining/60:.1f}åˆ†é’Ÿ")
                logger.info(f"{'â”€'*70}\n")
        
        # æœ€ç»ˆç»Ÿè®¡
        total_time = time.time() - start_time
        
        logger.info(f"\n{'='*70}")
        logger.info("ğŸ‰ å…¨é‡åŒæ­¥å®Œæˆï¼")
        logger.info(f"{'='*70}")
        logger.info(f"âœ… åŒæ­¥æˆåŠŸ: {success_count} åª")
        logger.info(f"ğŸ“Œ å·²æ˜¯æœ€æ–°: {up_to_date_count} åª")
        logger.info(f"âŒ åŒæ­¥å¤±è´¥: {failed_count} åª")
        logger.info(f"ğŸ“Š æ–°å¢è®°å½•: {total_inserted:,} æ¡")
        logger.info(f"â±ï¸  æ€»ç”¨æ—¶: {total_time/60:.1f} åˆ†é’Ÿ ({total_time/3600:.2f} å°æ—¶)")
        if len(stocks) > 0:
            logger.info(f"âš¡ å¹³å‡é€Ÿåº¦: {total_time/len(stocks):.1f} ç§’/è‚¡")
        logger.info(f"{'='*70}\n")
    
    def run_test_mode(self, symbol: str):
        """æµ‹è¯•æ¨¡å¼ï¼šåŒæ­¥å•åªè‚¡ç¥¨"""
        
        logger.info("="*70)
        logger.info(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šåŒæ­¥å•åªè‚¡ç¥¨ {symbol}")
        logger.info("="*70)
        
        result = self.sync_single_stock(symbol)
        
        if result.get('task', {}).get('status') == 'success':
            task_result = result['task']['result']
            logger.info(f"\nâœ… åŒæ­¥æˆåŠŸï¼")
            logger.info(f"   æ“ä½œ: {task_result.get('action')}")
            logger.info(f"   æ–°å¢è®°å½•: {task_result.get('inserted_count', 0)} æ¡")
            logger.info(f"   æ€»è®°å½•æ•°: {task_result.get('total_records', 0)} æ¡")
            logger.info(f"   æœ€æ–°æ—¥æœŸ: {task_result.get('latest_sync_date', 'N/A')}")
        else:
            logger.error(f"\nâŒ åŒæ­¥å¤±è´¥")
            logger.error(f"   é”™è¯¯: {result.get('task', {}).get('result', {}).get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        logger.info(f"{'='*70}\n")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å…¨é‡è‚¡ç¥¨åŒæ­¥è„šæœ¬ v2')
    parser.add_argument('--test', type=str, help='æµ‹è¯•æ¨¡å¼ï¼šæŒ‡å®šè‚¡ç¥¨ä»£ç ï¼ˆå¦‚: SH.600519ï¼‰')
    parser.add_argument('--max', type=int, help='æœ€å¤§åŒæ­¥æ•°é‡')
    parser.add_argument('--skip', type=int, default=0, help='è·³è¿‡å‰Nåª')
    parser.add_argument('--sync-url', type=str, default='http://localhost:7777/api', 
                        help='åŒæ­¥æœåŠ¡URLï¼ˆé»˜è®¤: http://localhost:7777/apiï¼‰')
    parser.add_argument('--etf', action='store_true', 
                        help='åŒæ­¥ETFä»·æ ¼è€Œéè‚¡ç¥¨')
    
    args = parser.parse_args()
    
    client = FullSyncClient(sync_url=args.sync_url)
    
    if args.test:
        # æµ‹è¯•æ¨¡å¼
        client.run_test_mode(args.test)
    elif args.etf:
        # ETFä»·æ ¼åŒæ­¥æ¨¡å¼ï¼ˆé€åªåŒæ­¥ï¼‰
        client.run_etf_sync(max_etfs=args.max, skip_count=args.skip)
    else:
        # è‚¡ç¥¨åŒæ­¥æ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        client.run_full_sync(max_stocks=args.max, skip_count=args.skip)


if __name__ == "__main__":
    main()
